{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretischer Hintergrund\n",
    "\n",
    "Was ist gerade der neueste Technologie Trend? Wie wird ein bestimmtes Produkt bewertet? Wie beurteilt die Gesellschaft aktuelle politische Themen? Sind Katzen Bilder immer noch süß oder nerven sie doch eher? <br>\n",
    "All diese und noch viele weitere spannende Fragen lassen sich durch ein Untergebiet des **Text Minings**, der **Sentimentanalyse** beantworten.<br>\n",
    "Wenn aus Texten mit statistischen und linguistischen Mitteln die relevanten Kerninformationen herausgefiltert werden, wird das Text Mining genannt.<br>\n",
    "Das Wort *Sentiment* stammt aus dem Französischen, und bedeutet soviel wie Gefühl oder Empfindung. Dem Zufolge wird bei der Sentimentanalyse also eine Empfindung gegenüber einer bestimmten Sache untersucht. \n",
    "\n",
    "\n",
    "### Zweck der Sentimentanalyse\n",
    "Die Sentimentanalyse ermittelt Stimmungen in sozialen Medien, bezogen auf Produkte, Kampagnen und Serviceleistungen. Dies gibt beispielsweise Unternehmen die Chance die Ursache von schlechten Meinungen und negativem Feedback zu analysieren und angemessen darauf zu reagieren. Des Weiteren könnnen sich Produkthersteller oder Dienstleistungsunternehmen einen Marktvorsprung dadurch generieren, dass sie bereits zukünftige Trends vor allen anderen erfahren.\n",
    "\n",
    "\n",
    "### Beispiel\n",
    "Ein Softwareunternehmen könnte beispielsweise diverse online Quellen, bei denen es hauptsächlich um Tech-Themen geht, anbinden, um von dort Daten abzuziehen. Diese Daten können dann genutzt werden, um herauszufinden, über was potenzielle Kunden aktuell diskutieren, welche Trends im Kommen sind und welche schon wieder am Verschwinden sind. So können sich Unternehmen noch besser den Bedürnissen Ihrer Kunden anpassen.\n",
    "\n",
    "### Wie funktioniert die Sentimentanalyse\n",
    "Bei der Sentimentanalyse werden Texte entweder *positiv,negativ* oder *neutral* klassifiziert.<br>\n",
    "Um diese Einteilung erzeugen zu können ist einige Vorarbeit nötig, wie zum Beispiel das Entfernen von Hyperlinks und Satzzeichen.\n",
    "Python stellt die Packages **NLTK** und **TextBlob** zur Verfügung, mit deren Hilfe ein Classifier implementiert werden kann. <br>Dieser lädt die Daten zunächst in einen Dataframe, um anschließend *Stopwords* zu entfernen und alle Buchstaben in Kleinbuchstaben zu konvertieren. Das Ergebnis ist ein gesäuberter Text. <br>Mit Hilfe der TextBlob Funktion kann die **Polarität** gemessen werden. Die Polarität ist ein Wert im Bereich zwischen $[-1.0,1.0]$ , wobei für Werte W folgendes gilt:<br> $ wi > 0 =    +(positiv) $<br> und<br>  $ wi < 0   =   - (negativ) $ <br>\n",
    "\n",
    "\n",
    "### Was kann analysiert werden?\n",
    "Die Sentimentanalyse kann in folgenden Bereichen angewendet werden:\n",
    "\n",
    "* Produktbewertungen\n",
    "* Service-/Dienstleistungsbewertungen\n",
    "* Kundengespräche\n",
    "* Trainingsdaten für maschinelles Lernen\n",
    "* Videos\n",
    "* Podcasts\n",
    "* Bilder und Grafiken\n",
    "* Soziale Medien:\n",
    "    * Tweets\n",
    "    * Facebookeinträge\n",
    "    * Blogkommentare\n",
    "    * Foreneinträge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aber wie kommen wir nun an die Daten?\n",
    "Für das folgende Beispiel werden Twitterdaten verwendet. Diese wurden über die Twitter Streaming API gepulled.\n",
    "Um diese nutzen zu können, muss ein [API Key](https://developer.twitter.com/en.html) angefordert werden.\n",
    "Um auf die Twitter API zugreifen zu können, kann die Python Library [Tweepy](http://www.tweepy.org/) genutzt werden. <br>\n",
    "Diese stellt Funktionen zur Verfügung, durch welche der Datenabzug parameterisiert und dadurch vereinfacht werden kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenspeicherung\n",
    "Um die Daten zu speichern bietet sich die dokumentenbasierte NoSQL Datenbank **MongoDB** an, da diese schnell zu installieren und einfach zu bedienen ist. <br> Diese kann kostenfrei auf [mongodb.com](https://www.mongodb.com/download-center#community) heruntergeladen werden. Hauptsächlich wird die MongoDB über die Konsole gesteuert, es gibt aber auch die Möglichkeit mit einer GUI zu arbeiten. Hierfür bietet sich der [Robomongo](https://robomongo.org/download) an.<br>\n",
    "Python bietet das Package **Pymongo** an, um direkt mit der MongoDB zu kommunizieren. Das vereinfacht die Datenanalyse enorm, da nicht immer csv Dateien generiert und eingelesen werden müssen, sondern aktuelle Daten direkt aus der Datenbank extrahiert werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenbankverbindung\n",
    "Nachdem alle Vorbereitungen getroffen wurden, geht es nun an den Datenabzug. Um uns mit der MongoDB verbinden zu können, benötigen wir den MongoClient aus dem Package Pymongo. Um die Twitter API zu nutzen benötigen wir das Package Tweepy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Die folgende Tweepy Klasse wurde implementiert um englischsprachige Tweets mit dem Tag **'Android'** abzuziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to mongoDB\n",
    "MONGO_HOST= 'mongodb://localhost:27017/DataScience'\n",
    "# words to search for within the tweets\n",
    "WORDS = ['#android' ]\n",
    "\n",
    "# authentication\n",
    "CONSUMER_KEY = \"INSERT_CONSUMER_KEY\"\n",
    "CONSUMER_SECRET = \"INSERT_CONSUMER_SECRET\"\n",
    "ACCESS_TOKEN = \"INSERT_ACCESS_TOKEN\"\n",
    "ACCESS_TOKEN_SECRET = \"INSERT_TOKEN_SECRET\"\n",
    "\n",
    "#This is a class provided by tweepy to access the Twitter Streaming API. \n",
    "class StreamListener(tweepy.StreamListener):    \n",
    "\n",
    "    # Called initially to connect to the Streaming API\n",
    "    def on_connect(self):\n",
    "    \n",
    "        print(\"You are now connected to the streaming API.\")\n",
    "    # On error - if an error occurs, display the error / status code\n",
    "    def on_error(self, status_code):\n",
    "    \n",
    "        print('An Error has occured: ' + repr(status_code))\n",
    "        return False\n",
    "    #connect to mongoDB and store the tweets\n",
    "    def on_data(self, data):\n",
    "   \n",
    "        try:\n",
    "            client = MongoClient(MONGO_HOST)\n",
    "            \n",
    "            # Use DataScience Database\n",
    "            db = client.DataScience\n",
    "    \n",
    "            # Decode the JSON from Twitter\n",
    "            datajson = json.loads(data)\n",
    "            \n",
    "            #grab the 'created_at' data from the Tweets to use for display\n",
    "            created_at = datajson['created_at']\n",
    "\n",
    "            #print out a message to the screen that we have collected a tweet\n",
    "            print(\"Tweet collected at \" + str(created_at))\n",
    "            \n",
    "            #insert the data into the mongoDB into a collection called Android\n",
    "            db.Android.insert_many(datajson)\n",
    "        except Exception as e: print(e)\n",
    "#Authentification via consumer key\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "#Set up the listener. The 'wait_on_rate_limit=True' is needed to help with Twitter API rate limiting.\n",
    "listener = StreamListener(api=tweepy.API(wait_on_rate_limit=True)) \n",
    "streamer = tweepy.Stream(auth=auth, listener=listener)\n",
    "print(\"Tracking: \" + str(WORDS))\n",
    "#Filter tweets with defined tags and in english language\n",
    "#streamer.filter(track=WORDS,languages =[\"en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ausgeführte Programm gibt folgendes aus:\n",
    "![tweepy](img/tweepy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten werden gestreamed und in der zuvor definierten Datenbank gespeichert:\n",
    "<br>\n",
    "\n",
    "<img src=\"img/mongodb.png\" alt=\"mongodb\" style=\"width: 600px\",style=\"align:left\";/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Sentimentanalyse durchführen zu können, müssen weitere Packages importiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend wird die Verbindung zur MongoDB hergestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection to MongoDB\n",
    "CLIENT = MongoClient('localhost', 27017)\n",
    "db = CLIENT['DataScience']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor der Classifier zum Einsatz kommt, müssen die Tweets noch gecleaned werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all', 'herself', 'was', 'here', 'then', 'have', 'or', 'this', 'any', 'own', \"isn't\", 'than', 't', 'for', 'll', 'i', 'm', 'yours', 'some', \"she's\", 'them', 'whom', \"haven't\", 'with', \"you'd\", \"wouldn't\", 'a', 'as', 'hasn', 'up', 'from', \"don't\", 'don', 'themselves', 'into', 'too', 'few', 'but', 'they', 'down', 'were', 'should', 'shan', \"aren't\", 'at', 'because', 'you', 'wouldn', 'doing', \"that'll\", \"mightn't\", 'of', 've', 'we', 'he', 'theirs', 'both', 'only', 'between', 'did', 'when', 'above', 'that', 'very', \"should've\", 'who', 'what', 'shouldn', 'by', 'now', 'in', \"it's\", 'out', 'nor', 'each', 'how', 'is', 'after', 'so', \"shouldn't\", 'below', 'myself', 'ourselves', 'yourself', 'her', 'until', 'o', 'most', 'same', 'against', 'hadn', 'over', 'under', 'an', 'ours', 'if', \"didn't\", 'me', \"hadn't\", 'been', 'can', 'during', 'their', 'before', 'am', 'on', 'it', 'has', 'further', 'himself', 'such', 'does', \"you've\", 'just', \"you're\", 'be', 'weren', 'ma', \"won't\", 'mightn', 'him', 'hers', 'couldn', 'doesn', 'isn', 'through', 'won', 'wasn', 'his', 'aren', 'our', 'the', 'having', 'other', 'do', \"weren't\", 'no', 'being', 'not', 'my', 'off', 'needn', 'itself', 'while', \"shan't\", 'once', 'there', 'had', 'about', 'are', 'haven', 'yourselves', 'those', \"you'll\", \"couldn't\", \"wasn't\", 'to', 'again', 'will', 'ain', 'she', 'which', 'y', 's', \"mustn't\", 'where', 'mustn', 'why', 'didn', \"doesn't\", 'these', 'your', 'd', \"hasn't\", \"needn't\", 're', 'and', 'its', 'more'}\n"
     ]
    }
   ],
   "source": [
    "#definition of stopwords and the vocabulary\n",
    "from nltk.corpus import stopwords\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das folgende Modul berechnet die Polarität für jeden Tweet und klassifiert diese so zu bestimmten Sentimenten. <br> Die Tweets erhalten alle ein Label bezüglich ihres Sentiments:<br>\n",
    "* 1 für positiv\n",
    "* 0 für neutral\n",
    "* -1 für negativ\n",
    "<br> \n",
    "\n",
    "Anschließend wird die Collection geupdated. Dies geschieht indem die neuen Labels den Datenbankeinträgen hinzugefügt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier():\n",
    "    \"\"\"Classifier for the sentiment of documents\"\"\"\n",
    "    \n",
    "    \n",
    "    def make_df(self, collection, parameter):\n",
    "        \"\"\"Loads the data form the given collection\n",
    "        \n",
    "        Args: \n",
    "            collection(pymongo.collection): Collection from which the data is loaded\n",
    "\t        parameter(str): Name of the field in which the text is stored\n",
    "        \n",
    "       \n",
    "        Returns: \n",
    "            pandas.dataframe: Data frame of the loaded data \n",
    "                \n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(list(collection.find({\"Sentiment\":{\"$exists\": False}},{parameter:1, \"_id\": 0})))\n",
    "        print(\"data is loaded: \"+str(len(df)))\n",
    "        return df\n",
    "    def load_text(self,df,parameter):\n",
    "        \"\"\"loads text out of a dataframe\n",
    "        \n",
    "        Args:   \n",
    "\t        df(pandas.dataframe): dataframe containing the data \n",
    "\t        parameter(str): name of the field where the text is stored\n",
    "        \n",
    "        Returns: \n",
    "            List of the texts stored in the data frame\n",
    "        \"\"\"\n",
    "        return df[parameter].tolist()\n",
    "    def update(self, collection, text, name, data, source):\n",
    "        \"\"\"Updates a document\n",
    "        \n",
    "        Args:\n",
    "            collection(pymongo.collection): collection, that is updated\n",
    "\t        text(str): text to identify the document\n",
    "\t        name(str): field name of the stored value\n",
    "\t        data(no specific datatype): value to be stored\n",
    "               \n",
    "        Returns: \n",
    "            None\n",
    "        \"\"\"\n",
    "        collection.update_one({source : text}, {'$set': {name: data}})\n",
    "    def process_text(self, text):\n",
    "        \"\"\"Preprocesses a given text\n",
    "        \n",
    "        Args: \n",
    "            text(str): text, that is preprocessed\n",
    "        \n",
    "        Returns: \n",
    "            tokens: tokens of a text \n",
    "        \n",
    "        \"\"\"\n",
    "        if text.startswith('@null'):\n",
    "            return \"[Text not available]\"\n",
    "        # Remove tickers\n",
    "        text = re.sub(r'\\$\\w*','',text)\n",
    "        # Remove hyperlinks\n",
    "        text = re.sub(r'https?:\\/\\/.*\\/\\w*','',text)\n",
    "        # Remove puncutations like \"'s\"\n",
    "        text = re.sub(r'['+string.punctuation+']+', ' ',text) \n",
    "        twtok = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        tokens = twtok.tokenize(text)\n",
    "        #if i not in stopwords and len(i) > 2 and i in english_vocab]\n",
    "        tokens = [i.lower() for i in tokens] \n",
    "        return tokens\n",
    "    def clean_texts(self, df,parameter):\n",
    "        \"\"\"cleans the texts of a data frame and stores the cleaned texts\n",
    "        \n",
    "        Args:\n",
    "            df(pandas.dataframe): dataframe which texts are cleaned\n",
    "\t        paramter(str): name of the field where the text is stored\n",
    "           \n",
    "        Returns:\n",
    "            list: List of the cleaned texts\n",
    "            list: list of the original texts \n",
    "        \"\"\"\n",
    "        texts = self.load_text(df,parameter)\n",
    "        # store all clean words \n",
    "        words = []\n",
    "        for t in texts:\n",
    "            words += self.process_text(t)\n",
    "            \n",
    "        #store the cleaned texts\n",
    "        cleaned_texts = []\n",
    "        for text in texts:\n",
    "            words = self.process_text(text)\n",
    "            #Form sentences of processed words\n",
    "            cleaned_text = \" \".join(w for w in words if len(w) > 2 and w.isalpha())\n",
    "            cleaned_texts.append(cleaned_text)\n",
    "        df['CleanText'] = cleaned_texts\n",
    "        return cleaned_texts, texts\n",
    "    def analyse_sentiment(self,text):\n",
    "        \"\"\"Classifies the sentiment of a text\n",
    "        \n",
    "        Args: \n",
    "            text(str): text, that is classified\n",
    "        \n",
    "        Returns: \n",
    "            int: Sentiment, -1 for negative, 0 for neutral, 1 for positive\n",
    "        \n",
    "        \"\"\"\n",
    "        analysis = TextBlob(text)\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    def update_sentiment(self, collection, parameter, df):\n",
    "        \"\"\"Analyses the sentiment and stores the results for the documents in a collection\n",
    "        \n",
    "        Args: \n",
    "            collection(pymongo.collection): collection, that is analyzed\n",
    "\t        paramter(str): name of the fields where the text is stored\n",
    "\t        df(pandas.dataframe): dataframe of the collection\n",
    "        \n",
    "        \n",
    "        Returns: \n",
    "            None\n",
    "        \n",
    "        \"\"\"\n",
    "        cleaned_texts, texts = self.clean_texts(df, parameter)\n",
    "        sentimented_texts = []\n",
    "        counter = 0\n",
    "        print(\"Hooray data is now cleaned\")\n",
    "        for tw in cleaned_texts:\n",
    "            #analysis the sentiment\n",
    "            sentiment = self.analyse_sentiment(tw) \n",
    "            sentimented_texts.append(sentiment)\n",
    "            #update of sentimented texts\n",
    "            self.update(collection, texts[counter], 'Sentiment', sentiment, parameter) \n",
    "            counter +=1\n",
    "        df['Sentiment'] = sentimented_texts\n",
    "    \n",
    "        print(\"Data is classified according to sentiments\")\n",
    "        pos_texts = [ text for index, text in enumerate(df[\"CleanText\"]) if df['Sentiment'][index] > 0]\n",
    "        neu_texts = [ text for index, text in enumerate(df[\"CleanText\"]) if df['Sentiment'][index] == 0]\n",
    "        neg_texts = [ text for index, text in enumerate(df[\"CleanText\"]) if df['Sentiment'][index] < 0]\n",
    "    \n",
    "        print(\"Percentage of positive tweets: {}%\".format(len(pos_texts)*100/len(df[parameter])))\n",
    "        print(\"Percentage of neutral tweets: {}%\".format(len(neu_texts)*100/len(df[parameter])))\n",
    "        print(\"Percentage of negative tweets: {}%\".format(len(neg_texts)*100/len(df[parameter])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #classify the sentiment of Twitter\n",
    "    c = SentimentClassifier()   \n",
    "    #p = \"text\"\n",
    "    #collection = db[\"Android\"]\n",
    "    #df_T = c.make_df(collection,p)\n",
    "    #tweets = c.load_text(df_T,p)\n",
    "    #c.update_sentiment(collection, p, df_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis zeigt zwar deutlich, dass *Android* offenbar als überwiegend positiv empfunden wird, <br>jedoch lässt die gleichzeit hohe Prozenzzahl von neutralen Postings die Vermutung zu, <br>\n",
    "dass der Classifier noch besser trainiert werden müsste um ein noch genaueres Ergebnis zu bekommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
